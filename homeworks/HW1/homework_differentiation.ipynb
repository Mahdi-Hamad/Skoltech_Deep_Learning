{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it easy to google every task please please please try to undestand what's going on. The \"just answer\" thing will be not counted, make sure to present derivation of your solution. It is absolutely OK if you found an answer on web then just exercise in $\\LaTeX$ copying it into here. A good way to derive solutions for these tasks is to derive it for single elements and then generalize to the resulting matrix/vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links: \n",
    "[1](http://www.machinelearning.ru/wiki/images/2/2a/Matrix-Gauss.pdf)\n",
    "[2](http://www.atmos.washington.edu/~dennis/MatrixCalculus.pdf)\n",
    "[3](http://cal.cs.illinois.edu/~johannes/research/matrix%20calculus.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex. 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scalar w.r.t. vector:\n",
    "$$  \n",
    "y = c^Tx,  \\quad x \\in \\mathbb{R}^N \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dy}{dx} = \n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ex.1 solution\n",
    "\n",
    "$$\n",
    "c^T x = \\sum_{i=1}^N c_i x_i \\Rightarrow\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x_i} = \n",
    "\\frac{\\partial(c_1x_1 + \\dots + c_ix_i + \\dots + c_Nx_N)}{\\partial x_i} = c_i \\;\\; \\forall i={1,...N}.\n",
    "$$\n",
    "So it total:\n",
    "$$\n",
    "\\frac{dy}{dx} = c^T, \\quad c^T \\in \\mathbb{R}^{1\\times N} .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex. 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector w.r.t. vector:\n",
    "$$ y = \\sum_{j=1}^{N} cx^T \\quad c \\in \\mathbb{R}^{M} ,x \\in \\mathbb{R}^{N}, cx^T \\in \\mathbb{R}^{M \\times N} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{dy}{dx} =\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ex.2 solution\n",
    "\n",
    "$$\n",
    "y_i = c_i \\sum_{j=1}^N x_j\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial x_j} = \n",
    "\\frac{c_i\\partial(x_1 + \\dots + x_j + \\dots + x_N)}{\\partial x_j} = c_i \\;\\; \\forall i=1...M.\n",
    "$$\n",
    "Let's denote $\\mathbb{1}$ as vector of ones and of size $1\\times N$. Finally:\n",
    "$$\n",
    "\\frac{dy}{dx} = c \\mathbb{1}, \\;\\; c \\mathbb{1} \\in \\mathbb{R}^{M \\times N}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex. 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector w.r.t. vector:\n",
    "$$  \n",
    "y = x x^T x , x \\in \\mathbb{R}^{N}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} =\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ex.3 solution\n",
    "$$\n",
    "y_i = x_i\\sum_{j=1}^N x_jx_j = x_i\\sum_{j=1}^N x_j^2.\n",
    "$$\n",
    "So:\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial x_j} = \n",
    "\\frac{\\partial \\bigg(x_i\\sum_{k=1}^N x_k^2 \\bigg)}{\\partial x_j}.\n",
    "$$\n",
    "First case when $i \\neq j$:\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial x_j} = \n",
    "x_i\\frac{\\partial (x_1^2 + \\dots + x_j^2 + \\dots + x_N^2 )}{\\partial x_j} = 2 x_i x_j.\n",
    "$$\n",
    "Second case when $i=j$:\n",
    "$$\n",
    "\\frac{\\partial y_i}{\\partial x_j} =\n",
    "\\frac{\\partial \\bigg(x_i (x_1^2 + \\dots + x_j^2 + \\dots + x_N^2 )\\bigg)}{\\partial x_j} =\n",
    "2 x_i^2 + \\sum_{k=1}^N x_k^2.\n",
    "$$\n",
    "\n",
    "Let's denote $I$ as identity matrix of size $N \\times N$. Finally:\n",
    "$$\n",
    "\\frac{dy}{dx} = x^T x I + 2 x x^T, \\;\\; \\frac{dy}{dx} \\in \\mathbb{R}^{N \\times N}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ex. 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivatives for the parameters of the Dense layer:\n",
    "\n",
    "***Given :***  $$Y = XW, Y \\in \\mathbb{R}^{N \\times OUT}, X \\in \\mathbb{R}^{N \\times IN}, W \\in \\mathbb{R}^{IN \\times OUT} $$ \n",
    "\n",
    "The derivative of the hypothetic loss function w.r.t. to $Y$ is known: $\\Delta Y  \\in \\mathbb{R}^{N \\times OUT}$\n",
    "\n",
    "***Task :*** Please, derive the gradients of the loss w.r.t the weight matrix $W$: $\\Delta W  \\in \\mathbb{R}^{IN \\times OUT}$. Use the chain rule. First, please, derive each element of the $\\Delta W$, then generalize to the matrix form.\n",
    " \n",
    "Useful link: http://cs231n.stanford.edu/vecDerivs.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ex.4 solution\n",
    "\n",
    "$$\n",
    "Y_{ab} = \\sum_{i=1}^{IN} X_{ai} W_{ib} \\Rightarrow\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial Y_{ab}}{\\partial W_{ij}} = \n",
    "\\frac{\\partial \\bigg( \\sum_{k=1}^{IN} X_{ak} W_{kb}\\bigg)}{\\partial W_{ij}}.\n",
    "$$\n",
    "\n",
    "So as in the previous task. Forst case when $b \\neq j$:\n",
    "$$\n",
    "\\frac{\\partial Y_{ab}}{\\partial W_{ij}} = 0.\n",
    "$$\n",
    "Second case when $b = j$:\n",
    "$$\n",
    "\\frac{\\partial Y_{ab}}{\\partial W_{ij}} = X_{aj}.\n",
    "$$\n",
    "Chain rule: \n",
    "$$\n",
    "\\frac{\\partial loss}{\\partial W} = \\frac{\\partial loss}{\\partial Y}\\frac{\\partial Y}{\\partial W}.\n",
    "$$\n",
    "So in result:\n",
    "$$\n",
    "[\\frac{\\partial loss}{\\partial W}]_{ij} = \n",
    "\\sum_{a, b}^{N, out} \\frac{\\partial loss}{\\partial Y_{ab}} \\frac{\\partial Y_{ab}}{\\partial W_{ij}} =\n",
    "\\sum_{a=1}^{N} \\Delta Y_{aj} X_{aj} \\Rightarrow\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial loss}{\\partial W} = X^T \\Delta Y, \\;\\; \\frac{\\partial loss}{\\partial W} \\in \\mathbf{R}^{IN \\times OUT}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
